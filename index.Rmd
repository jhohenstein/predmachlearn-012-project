---
author: Jeff Hohenstein
output: 
  html_document :
    toc: true
    theme: cosmo
---

# Predicting Bicep Curl Quality via Random Forest

### Introduction

Sensor data was collected by the [Human Activity Recognition](http://groupware.les.inf.puc-rio.br/har) project of GroupWare during various exercises for the purpose of building models to automate the recognition of exercise quality. The data set under study here was for bicep curls. Two data sets, a training and a testing data set were provided. A predictive model was constructed using random forests. The final model achieved an in-model accuracy of (TODO). When measured against a separate testing data set, the model gave a 99.2% accuracy. This is an exceptional result considering that prediction of movement quality is a novel area of research and that typical model accuracies in more mature fields are lower.

### Required Libraries

```{r}
library(caret)
library(rattle)
library(ggplot2)
```

### Data Processing

Two data sets were provided, a [training data set](http://d396qusza40orc.cloudfront.net/predmachlearn/pml­training.csv) and a [testing data set](http://d396qusza40orc.cloudfront.net/predmachlearn/pml­testing.csv). 

```{r,cache=TRUE}
training <- read.csv("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",na.strings=c("NA","#DIV/0!"),stringsAsFactors=FALSE)
testing <- read.csv("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",na.strings=c("NA","#DIV/0!"),stringsAsFactors=FALSE)
```

The training data set contained `r dim(training)[1]` observations on `r dim(training)[2]` variables including an output quality classification of A, B, C, D, E. 

Many of the variables in the data set contain mostly NAs. 

```{r,cache=TRUE}
NAs <- apply(training,2,function(v) sum(is.na(v)))
hist(NAs)
```

Though there might have been some information useful to model building, these were removed to speed construction. 

```{r,cache=TRUE}
cleanPredictors <- function(data){
  
  clean <- data
  
  hasClass <- sum(names(data)=="classe")
  
  if(hasClass){clean[,-which(names(data) %in% c("classe"))]}
  
  # remove columns not used for prediction
  clean <- clean[,-which(names(data) %in% c("X","user_name","cvtd_timestamp","raw_timestamp_part_1","raw_timestamp_part_2","num_window","new_window"))]
  
  # Remove columns with lots of NAs
  good_columns <- apply(is.na(clean),2,sum)<10
  clean <- clean[,good_columns]
  
  # Convert remaining columns to numeric
  clean <- sapply(clean,as.numeric)
  
  clean <- data.frame(clean)
  
  # Add classe back to the frame
  if(hasClass){clean$classe <- factor(data$classe)}
  
  clean
  
}

cleanTraining <- cleanPredictors(training)
cleanTesting <- cleanPredictors(testing)
```

In addition, non-numeric variables, such as name, timestamp, etc. were removed from the data. 

The final data set, prior to model building was reduced to 52 predictors and a single output variable.

### Model Construction

Initial model building focused on classification and regression trees given that this is a classification problem. 

```{r,cache=TRUE}
cart <- train(classe ~ ., data=cleanTraining, method="rpart")
fancyRpartPlot(cart$finalModel)
```

Models built this way yielded relatively poor accuracy `r max(cart$results$Accuracy)` but gave some insight into which predictors were most important in classifying the output.

After CART, Random Forests were selected as the final model building approach. 

The training data set was divided into two partitions, a training partition and a validation partition with a 60/40 split. Random forests do internal "out-of-bag" estimation during model building but the validation partition gives an extra sense of confidence.

```{r}
set.seed(324565)
trainSet <- createDataPartition(cleanTraining$classe,p = .6,times=1,list=FALSE)
trainPart <- cleanTraining[trainSet,]
testPart <- cleanTraining[-trainSet,]
```


The training partition was `r dim(trainPart)[1]` observations. The validation partition was `r dim(testPart)[1]` observations. 

Model building was performed with the default 500 trees and using the default settings for number of predictors. Cross-validation was 5-fold in which the Random Forest algorithm internally partitions the data into 5 folds and uses 1 fold to assess the quality of the others.

```{r,eval=FALSE}
rfControl <- trainControl(
    method="cv",
    number=5,
    allowParallel=TRUE
  )

rfFit <- train(
  classe ~ .,
  data=trainPart,
  method="rf",
  importance=TRUE,
  proximity=TRUE,
  do.trace=TRUE,
  trControl = rfControl)
```

Load the precomputed model.

```{r}
load(file="rfFit.model")
```

### Model Quality

The final accuracy rate reported by the training algorithm as OOB Accuracy was `r rfFit$results$Accuracy[rfFit$results$mtry==rfFit$bestTune[[1]]]`. 

```{r}
rfFit$results
```

Following is the confusion matrix for the final model reported using the OOB cross-validation.

```{r,echo=FALSE}
rfFit$finalModel$confusion
```

Following is the confusion matrix for the validation partition.

```{r}
p <- predict(rfFit, newdata = testPart)
c <- confusionMatrix(p,testPart$classe)
c
```

The estimated accuracy rate using the validation partition is `r round(c$overall[1],3)`. 

### Variable Importance

The following gives the top 5 variables for relative importance in predicting the classes.

```{r}
par(las=2)
barplot(head(rfFit$finalModel$importance[,7],n=10),ylab="Gini")
```

It is interesting that the variables presumably associated with overall posture as mesaured by the belt are more significant than those measured directly on the arm.

### Conclusion

Random forests did an exceptional job at predicting exercise quality based on accelerometer-based human activity measurements yielding a suprisingly high accuracy of `r round(c$overall[1],3)` given very modest up-front data manipulation. Training time was significant at around 2 hours for the final model but, once built, predictions were extremely fast. Supposing the model to be stable over time, this opens the possibility of portable, device-based assessments of exercise quality.
